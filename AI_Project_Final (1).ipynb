{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ZHz51pywt6xm"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "class HMM:\n",
        "  def __init__(self):\n",
        "    self.states = [\n",
        "    \"B-PER\",  # Beginning of a person named entity\n",
        "    \"I-PER\",  # Inside a person named entity\n",
        "    \"B-ORG\",  # Beginning of an organization named entity\n",
        "    \"I-ORG\",  # Inside an organization named entity\n",
        "    \"B-LOC\",  # Beginning of a location named entity\n",
        "    \"I-LOC\",  # Inside a location named entity\n",
        "    \"B-MISC\",  # Beginning of a miscellaneous named entity\n",
        "    \"I-MISC\",  # Inside a miscellaneous named entity\n",
        "    \"O\"       # Outside of any named entity\n",
        "    ]\n",
        "    self.dataset = []\n",
        "    self.transition_probs = defaultdict(lambda: defaultdict(float))\n",
        "    self.emission_probs = defaultdict(lambda: defaultdict(float))\n",
        "    self.initial_probs = defaultdict(float)\n",
        "\n",
        "  def process_dataset(self, filename):\n",
        "    \"\"\"\n",
        "    Process the CoNLL 2003 dataset.\n",
        "\n",
        "    Args:\n",
        "    - filename: The path to the CoNLL 2003 dataset file.\n",
        "\n",
        "    Returns:\n",
        "    - dataset: A list of tuples, where each tuple contains the first and last words from each line.\n",
        "    \"\"\"\n",
        "\n",
        "    with open(filename, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()  # Remove leading and trailing whitespace\n",
        "\n",
        "            # Skip lines starting with '-DOCSTART-'\n",
        "            if line.startswith(\"-DOCSTART-\"):\n",
        "                continue\n",
        "\n",
        "            # If line is empty (contains only newline character), add newline character to dataset\n",
        "            if not line:\n",
        "                self.dataset.append(('\\n', '\\n'))\n",
        "            else:\n",
        "                # Split the line into words\n",
        "                words = line.split(' ')\n",
        "\n",
        "                # Extract the first and last words\n",
        "                first_word = words[0]\n",
        "                last_word = words[-1]\n",
        "\n",
        "                self.dataset.append((first_word, last_word))\n",
        "\n",
        "  def train(self, filename):\n",
        "    \"\"\"\n",
        "    Train a Hidden Markov Model (HMM) using the given dataset.\n",
        "\n",
        "    Args:\n",
        "    - dataset: A list of tuples, where each tuple contains the observed word and its corresponding NER tag.\n",
        "\n",
        "    Returns:\n",
        "    - transition_probs: Transition probabilities between NER tags.\n",
        "    - emission_probs: Emission probabilities of words given NER tags.\n",
        "    - initial_probs: Initial probabilities of NER tags.\n",
        "    \"\"\"\n",
        "    self.process_dataset(filename)\n",
        "    transition_counts = defaultdict(lambda: defaultdict(int))\n",
        "    emission_counts = defaultdict(lambda: defaultdict(int))\n",
        "    initial_counts = defaultdict(int)\n",
        "    # Count occurrences of NER tags and transitions\n",
        "    prev_tag = None\n",
        "    for word, ner_tag in self.dataset:\n",
        "        if prev_tag is None or prev_tag == '\\n':\n",
        "            initial_counts[ner_tag] += 1\n",
        "        elif word == '\\n':\n",
        "            prev_tag = ner_tag\n",
        "            continue\n",
        "        else:\n",
        "            transition_counts[prev_tag][ner_tag] += 1\n",
        "        emission_counts[ner_tag][word] += 1\n",
        "        prev_tag = ner_tag\n",
        "\n",
        "    # Calculate initial probabilities\n",
        "    total_initial = sum(initial_counts.values())\n",
        "    for tag, count in initial_counts.items():\n",
        "        self.initial_probs[tag] = count / total_initial\n",
        "\n",
        "    # Calculate transition probabilities\n",
        "    for prev_tag, next_tags in transition_counts.items():\n",
        "        total_transitions = sum(next_tags.values())\n",
        "        for tag, count in next_tags.items():\n",
        "            self.transition_probs[prev_tag][tag] = count / total_transitions\n",
        "\n",
        "    # Calculate emission probabilities\n",
        "    for tag, word_counts in emission_counts.items():\n",
        "        total_emissions = sum(word_counts.values())\n",
        "        for word, count in word_counts.items():\n",
        "            self.emission_probs[tag][word] = count / total_emissions\n",
        "  def viterbi(self, sentence):\n",
        "    \"\"\"\n",
        "    Perform NER tagging on a given sentence using the Viterbi algorithm.\n",
        "\n",
        "    Args:\n",
        "    - sentence: A list of words in the sentence.\n",
        "\n",
        "    Returns:\n",
        "    - tagged_sentence: A list of tuples, where each tuple contains a word and its corresponding NER tag.\n",
        "    \"\"\"\n",
        "    if not hasattr(self, 'transition_probs') or not self.transition_probs \\\n",
        "        or not hasattr(self, 'emission_probs') or not self.emission_probs \\\n",
        "        or not hasattr(self, 'initial_probs') or not self.initial_probs:\n",
        "        raise RuntimeError(\"Model has not been trained. Please call the train method first.\")\n",
        "\n",
        "    # Define the time pattern\n",
        "    time_pattern = r'\\d{1,2}:\\d{2}(?:am|pm)?|\\d+(?:am|pm)'\n",
        "    time_regex = re.compile(time_pattern)\n",
        "\n",
        "    # Check if the sentence is empty\n",
        "    if not sentence:\n",
        "        return []\n",
        "\n",
        "    # Initialize matrices for dynamic programming\n",
        "    dp = [{} for _ in range(len(sentence))]\n",
        "    backpointer = [{} for _ in range(len(sentence))]\n",
        "\n",
        "    # Initialize initial probabilities\n",
        "    for state in self.states:\n",
        "        # Use a small non-zero value instead of directly using the emission probability\n",
        "        dp[0][state] = self.initial_probs[state] * self.emission_probs[state].get(sentence[0], 1e-10)\n",
        "        # Set the backpointer for the initial state to itself\n",
        "        backpointer[0][state] = state\n",
        "\n",
        "    # Iterate over each word in the sentence\n",
        "    for t in range(1, len(sentence)):\n",
        "        for state in self.states:\n",
        "            # Calculate the maximum probability and corresponding backpointer\n",
        "            max_prob = float('-inf')\n",
        "            max_state = None\n",
        "            for prev_state in self.states:\n",
        "                prob = dp[t - 1][prev_state] * self.transition_probs[prev_state][state]\n",
        "                if prob > max_prob:\n",
        "                    max_prob = prob\n",
        "                    max_state = prev_state\n",
        "            dp[t][state] = max_prob * self.emission_probs[state].get(sentence[t], 0)\n",
        "            backpointer[t][state] = max_state\n",
        "\n",
        "    # Backtrack to find the most likely sequence of states\n",
        "    best_path_prob = float('-inf')\n",
        "    best_path_end_state = None\n",
        "    for state in self.states:\n",
        "        if dp[-1][state] > best_path_prob:\n",
        "            best_path_prob = dp[-1][state]\n",
        "            best_path_end_state = state\n",
        "    best_path = [best_path_end_state]\n",
        "    for t in range(len(sentence) - 1, 0, -1):\n",
        "        best_path.append(backpointer[t][best_path[-1]])\n",
        "    best_path.append(backpointer[0][best_path[-1]])  # Add backpointer at index 0\n",
        "    best_path.reverse()\n",
        "\n",
        "    # Check if a word matches the time pattern and tag it as \"O\" if it does\n",
        "    tagged_sentence = []\n",
        "    for word, tag in zip(sentence, best_path):\n",
        "        if time_regex.match(word):\n",
        "            tag = \"O\"  # Tag as non-NER\n",
        "        tagged_sentence.append((word, tag))\n",
        "\n",
        "    return tagged_sentence\n",
        "\n",
        "\n",
        "\n",
        "  def viterbi_batch(self, filename):\n",
        "    \"\"\"\n",
        "    Perform NER tagging on a list of sentences using the Viterbi algorithm.\n",
        "\n",
        "    Args:\n",
        "    - sentences: A list of lists, where each inner list contains words in a sentence.\n",
        "\n",
        "    Returns:\n",
        "    - tagged_sentences: A list of tagged sentences, where each tagged sentence is a list of tuples\n",
        "      containing words and their corresponding NER tags.\n",
        "    \"\"\"\n",
        "    sentences = self.tokenize_file(filename)\n",
        "    tagged_sentences = []\n",
        "    for sentence in sentences:\n",
        "        tagged_sentence = self.viterbi(sentence)\n",
        "        tagged_sentences.append(tagged_sentence)\n",
        "    return tagged_sentences\n",
        "  def tokenize_file(self,filename):\n",
        "    \"\"\"\n",
        "    Tokenize the sentences in a file based on the CoNLL 2003 dataset.\n",
        "\n",
        "    Args:\n",
        "    - filename: The path to the file to be tokenized.\n",
        "\n",
        "    Returns:\n",
        "    - tokenized_sentences: A list of tokenized sentences.\n",
        "    \"\"\"\n",
        "    tokenized_sentences = []\n",
        "\n",
        "    # Regular expression to identify time expressions like \"12:30\", \"3pm\", \"3:30am\", etc.\n",
        "    time_pattern = r'\\d{1,2}:\\d{2}(?:am|pm)?|\\d+(?:am|pm)'\n",
        "\n",
        "    with open(filename, 'r') as file:\n",
        "        for line in file:\n",
        "            # Tokenize the sentence using whitespace as the delimiter\n",
        "            tokens = line.strip().split(' ')\n",
        "            # Initialize a list to store the tokenized words\n",
        "            tokenized_sentence = []\n",
        "            for token in tokens:\n",
        "                # Check if the token matches the time pattern\n",
        "                if re.match(time_pattern, token):\n",
        "                    # If it's a time expression, split it into separate tokens\n",
        "                    time_tokens = re.findall(r'\\d+|\\D+', token)\n",
        "                    tokenized_sentence.extend(time_tokens)\n",
        "                else:\n",
        "                    # Otherwise, treat the token as a single word\n",
        "                    # Handle punctuation marks, numbers, symbols, special characters, and quotation marks as separate words\n",
        "                    tokenized_sentence.extend(re.findall(r'\\w+|[^\\w\\s]', token))\n",
        "            # Add the tokenized sentence to the list\n",
        "            tokenized_sentences.append(tokenized_sentence)\n",
        "\n",
        "    return tokenized_sentences\n",
        "ner = HMM()\n",
        "ner.train('engtrain.txt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "aadarsh = \"Hello my name is Aadarsh\".split()\n",
        "print(aadarsh)\n",
        "print(ner.viterbi(aadarsh))\n",
        "tayaba = \"Hello i am Tayaba Jameel and i am crazy\".split()\n",
        "print(ner.viterbi(tayaba))\n",
        "blue = \"EU rejects German call to boycott British lamb .\".split()\n",
        "print(ner.viterbi(blue))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcyCILQoZbur",
        "outputId": "e2dd35b9-f8ae-45bf-f4fd-4309b11f8cd3"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'my', 'name', 'is', 'Aadarsh']\n",
            "[('Hello', 'O'), ('my', 'O'), ('name', 'O'), ('is', 'O'), ('Aadarsh', 'O')]\n",
            "[('Hello', 'B-ORG'), ('i', 'B-ORG'), ('am', 'I-ORG'), ('Tayaba', 'O'), ('Jameel', 'B-PER'), ('and', 'B-PER'), ('i', 'B-PER'), ('am', 'B-PER'), ('crazy', 'B-PER')]\n",
            "[('EU', 'B-ORG'), ('rejects', 'B-ORG'), ('German', 'O'), ('call', 'B-MISC'), ('to', 'O'), ('boycott', 'O'), ('British', 'O'), ('lamb', 'B-MISC'), ('.', 'O')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_dataset(filename):\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    with open(filename, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()  # Remove leading and trailing whitespace\n",
        "\n",
        "            # Skip lines starting with '-DOCSTART-'\n",
        "            if line.startswith(\"-DOCSTART-\"):\n",
        "                continue\n",
        "\n",
        "            # If line is empty (contains only newline character), add newline character to dataset\n",
        "            if not line:\n",
        "                sentences.append(sentence)\n",
        "                sentence = []\n",
        "            else:\n",
        "                # Split the line into words\n",
        "                words = line.split(' ')\n",
        "\n",
        "                # Extract the first word and append it as a list\n",
        "                first_word = words[0]\n",
        "                sentence.append(first_word)\n",
        "    return sentences\n",
        "\n",
        "data = process_dataset('engtesta.txt')\n",
        "tagged_sentences = []\n",
        "for sentence in data:\n",
        "  tagged_sentences.append(ner.viterbi(sentence))\n",
        "del tagged_sentences[0]\n",
        "\n",
        "def calculate_accuracy(predicted_tags, actual_tags):\n",
        "    \"\"\"\n",
        "    Calculate the accuracy of predicted NER tags compared to actual NER tags.\n",
        "\n",
        "    Args:\n",
        "    - predicted_tags: A list of predicted NER tags for each word in the test dataset.\n",
        "    - actual_tags: A list of actual NER tags for each word in the test dataset.\n",
        "\n",
        "    Returns:\n",
        "    - accuracy: The accuracy of the predicted NER tags.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for pred_sentence, actual_sentence in zip(predicted_tags, actual_tags):\n",
        "        for pred_tag, actual_tag in zip(pred_sentence, actual_sentence):\n",
        "            if pred_tag[1] == actual_tag[1]:  # Compare NER tags\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "def process_test_dataset(filename):\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    with open(filename, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()  # Remove leading and trailing whitespace\n",
        "\n",
        "            # Skip lines starting with '-DOCSTART-'\n",
        "            if line.startswith(\"-DOCSTART-\"):\n",
        "                continue\n",
        "\n",
        "            # If line is empty (contains only newline character), add newline character to dataset\n",
        "            if not line:\n",
        "                sentences.append(sentence)\n",
        "                sentence = []\n",
        "            else:\n",
        "                # Split the line into words\n",
        "                words = line.split(' ')\n",
        "\n",
        "                # Extract the first word and append it as a list\n",
        "                first_word = words[0]\n",
        "                last_word = words[-1]\n",
        "                sentence.append((first_word,last_word))\n",
        "    return sentences\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = process_test_dataset(\"engtesta.txt\")\n",
        "del test_dataset[0]\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = calculate_accuracy(predicted_tags, actual_tags)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sRumWzGhfeb",
        "outputId": "441271c8-e947-4ab3-c8f1-063fd6251919"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4307269966122815\n"
          ]
        }
      ]
    }
  ]
}